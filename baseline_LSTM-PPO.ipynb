{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRsMbJ9qkFop",
    "outputId": "8c5fee91-4965-4b36-bb5e-2b0ece46b773",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to install required modules\n",
    "\n",
    "#!pip install mxnet-cu101 opencv-python numpy tensorboard mxboard matplotlib pandas_bokeh gym[atari]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmWaxJlDkFow",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import cv2\n",
    "from mxnet import nd, gluon, init, autograd\n",
    "from mxnet.gluon import nn, rnn\n",
    "import mxnet as mx\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pandas_bokeh\n",
    "import gc\n",
    "import os\n",
    "import multiprocessing\n",
    "import multiprocessing.connection\n",
    "from mxboard import SummaryWriter\n",
    "import datetime\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Configuration\n",
    "output_name      = 'baseline'\n",
    "game             = \"SeaquestDeterministic-v4\"\n",
    "stacked_frames   = 4            # Number of stacked frames to use\n",
    "context          = mx.gpu()     # GPU or CPU based training\n",
    "opt_lr           = 2.5e-4       # Adam optimizer learning rate\n",
    "opt_eps          = 1e-5         # Adam optimizer epsilon value\n",
    "opt_clip         = .5           # Amount to clip gradients by\n",
    "actor_coeff      = 1.           # Loss coefficient of the actor loss (for scaling the different loss components)\n",
    "critic_coeff     = .5           # Loss coefficient of the critic loss\n",
    "entropy_coeff    = .02          # Loss coefficient of the entropy term\n",
    "hidden_size      = 512          # Number of latent hidden units before the LSTM layer\n",
    "rnn_hidden_size  = 1024         # Number of hidden units in the LSTM\n",
    "num_workers      = 16           # Number of parallel environments running\n",
    "batch_steps      = 125\n",
    "c, w, h          = 1, 84, 84\n",
    "gamma            = .99\n",
    "lamda            = .95\n",
    "clip_range       = 0.10\n",
    "schedule_steps   = 10000\n",
    "cooldown_period  = 200\n",
    "epochs           = 4\n",
    "n_mini_batch     = 5\n",
    "\n",
    "\n",
    "# Initialize other globals\n",
    "env             = gym.make(game)\n",
    "cur_eps         = np.zeros((num_workers), dtype=np.int32)\n",
    "batch_size      = num_workers * batch_steps\n",
    "mini_work_size  = batch_steps // n_mini_batch\n",
    "mini_batch_size = batch_size // n_mini_batch\n",
    "states          = np.zeros((num_workers, 1, stacked_frames*c, w, h), dtype=np.float32)\n",
    "states_new      = np.zeros((num_workers, 210, 160, 3), dtype=np.float32)\n",
    "lives           = np.zeros(num_workers, dtype=np.int32) + env.unwrapped.ale.lives()\n",
    "total_episodes  = 0\n",
    "all_grads       = []\n",
    "output_dir      = './logs/'+output_name+datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz5HKONykFox",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Colab tensorboard extension, uncomment if running in colab\n",
    "\n",
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AL4abphskFox",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Colab tensorboard, uncomment if running in Colab\n",
    "\n",
    "#%tensorboard --logdir './logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gugumLP7kFoy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define ResNet-based Encoder model\n",
    "\n",
    "class Encoder(gluon.Block):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.layers = {}\n",
    "            self.layers.items()\n",
    "            self.channel_list = [24,32,64,128]\n",
    "            for i, channels in enumerate(self.channel_list):\n",
    "                layer = str(i)\n",
    "                self.layers['conv'+layer] = nn.Conv2D(channels, 3, strides=1, padding=1)\n",
    "                self.layers['max'+layer] = nn.MaxPool2D(pool_size=3, strides=2,padding=1)\n",
    "                self.layers['res'+layer+'_0'] =  ResidualBlock(channels)\n",
    "                self.layers['res'+layer+'_1'] =  ResidualBlock(channels)\n",
    "\n",
    "            for key, val in self.layers.items():\n",
    "                self.register_child(self.layers[key])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, channels in enumerate(self.channel_list):\n",
    "            layer = str(i)\n",
    "            x = nd.relu(x)\n",
    "            x = self.layers['conv'+layer](x)\n",
    "            x = self.layers['max'+layer](x)\n",
    "            x = self.layers['res'+layer+'_0'](x)\n",
    "            x = self.layers['res'+layer+'_1'](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualBlock(gluon.Block):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        with self.name_scope():\n",
    "            self.conv0 = nn.Conv2D(in_channels, 3, strides=1, padding=1)\n",
    "            self.conv1 = nn.Conv2D(in_channels, 3, strides=1, padding=1)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = nd.relu(x)\n",
    "        h = self.conv0(h)\n",
    "        h = nd.relu(h)\n",
    "        h = self.conv1(h)\n",
    "        x = h+x\n",
    "        return x\n",
    "\n",
    "\n",
    "# Main model definition\n",
    "\n",
    "class Model(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.enc = Encoder()\n",
    "            self.dense = nn.Dense(hidden_size)\n",
    "            self.lstm = rnn.LSTMCell(rnn_hidden_size)\n",
    "            self.flat = nn.Flatten()\n",
    "            self.action = nn.Dense(env.action_space.n)\n",
    "            self.value = nn.Dense(1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size, timesteps, stacked_frames, H, W = x.shape\n",
    "        x = x.reshape(batch_size*timesteps, stacked_frames, H, W)\n",
    "        x = nd.relu(self.enc(x))\n",
    "        rnn_input = nd.tanh(self.dense(x))\n",
    "        input_and_action = nd.concat(rnn_input, nd.zeros((batch_size*timesteps,env.action_space.n), dtype=np.float32).as_in_context(context), dim = 1)\n",
    "        input_and_action = input_and_action.reshape(batch_size, timesteps, -1)\n",
    "        x, hidden = self.lstm.unroll(length=timesteps, inputs=input_and_action, layout='NTC', begin_state=hidden, merge_outputs=True)\n",
    "\n",
    "        rnn_output = x.reshape(batch_size*timesteps, -1)\n",
    "        probs = self.action(rnn_output)\n",
    "        values = self.value(rnn_output)\n",
    "        return nd.softmax(probs.astype(np.float64)).astype(np.float32), values, hidden, rnn_input, rnn_output\n",
    "\n",
    "    def encode(self, x):\n",
    "        batch_size, timesteps, stacked_frames, H, W = x.shape\n",
    "        x = x.reshape(batch_size*timesteps, stacked_frames, H, W)\n",
    "        encoded = nd.relu(self.enc(x))\n",
    "        encoded = nd.tanh(self.dense(encoded))\n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qs79QGcSkFo2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Multiprocessing setup\n",
    "\n",
    "class Game(object):\n",
    "    def __init__(self, game):\n",
    "        self.env = gym.make(game)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def step(self, action):\n",
    "        #self.env.render()\n",
    "        return self.env.step(action)\n",
    "\n",
    "\n",
    "def runner_process(remote, game):\n",
    "    game = Game(game)\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == \"step\":\n",
    "            remote.send(game.step(data))\n",
    "        elif cmd == \"reset\":\n",
    "            remote.send(game.reset())\n",
    "        elif cmd == \"close\":\n",
    "            remote.close()\n",
    "            break\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "class Runner:\n",
    "    def __init__(self, game):\n",
    "        self.child, parent = multiprocessing.Pipe()\n",
    "        self.process = multiprocessing.Process(target=runner_process, args=(parent, game))\n",
    "        self.process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVbkTQv1kFo3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# MxBoard / Tensorboard Monitoring Setup\n",
    "\n",
    "class Monitoring:\n",
    "    def __init__(self, output_dir):\n",
    "        self.update = []\n",
    "        self.total_episodes = []\n",
    "        self.rewards = []\n",
    "        self.mean100 = []\n",
    "        self.max_reward = []\n",
    "        self.update_max_reward = []\n",
    "        self.update_rewards = []\n",
    "        self.update_mean100 = []\n",
    "        self.critic_loss = []\n",
    "        self.actor_loss = []\n",
    "        self.entropy_loss = []\n",
    "        self.min_action_prob = []\n",
    "        self.max_action_prob = []\n",
    "        self.avg_action_prob = []\n",
    "        self.std_action_prob = []\n",
    "        self.avg_value = []\n",
    "        self.min_value = []\n",
    "        self.max_value = []\n",
    "        self.std_value = []\n",
    "        self.ratio = []\n",
    "        self.entropy_loss_buffer = []\n",
    "        self.actor_loss_buffer = []\n",
    "        self.critic_loss_buffer = []\n",
    "        self.ratio_buffer = []\n",
    "        self.sw = SummaryWriter(logdir=output_dir, flush_secs=5)\n",
    "\n",
    "\n",
    "    def process_episode(self, rewards, ep):\n",
    "        self.rewards.append(rewards)\n",
    "        self.total_episodes.append(ep)\n",
    "        if len(self.rewards)>100:\n",
    "            self.mean100.append(np.mean(self.rewards[-100:]))\n",
    "        else:\n",
    "            self.mean100.append(np.mean(self.rewards))\n",
    "        self.max_reward.append(np.max(self.rewards))\n",
    "\n",
    "    def process_rollout(self, data):\n",
    "        for key, val in data.items():\n",
    "            val = val.reshape(val.shape[0] * val.shape[1], *val.shape[2:])\n",
    "        self.min_action_prob.append(np.min(data['action_dists']))\n",
    "        self.max_action_prob.append(np.max(data['action_dists']))\n",
    "        self.avg_action_prob.append(np.mean(np.exp(data['log_probs'])))\n",
    "        self.std_action_prob.append(np.std(data['action_dists']))\n",
    "        self.avg_value.append(np.mean(data['values']))\n",
    "        self.min_value.append(np.min(data['values']))\n",
    "        self.max_value.append(np.max(data['values']))\n",
    "        self.std_value.append(np.std(data['values']))\n",
    "\n",
    "    def process_minibatch_loss(self, entropy_loss, actor_loss, critic_loss, ratio):\n",
    "        self.entropy_loss_buffer.append(entropy_loss)\n",
    "        self.actor_loss_buffer.append(actor_loss)\n",
    "        self.critic_loss_buffer.append(critic_loss)\n",
    "        self.ratio_buffer.append(ratio)\n",
    "\n",
    "    def process_update(self, update):\n",
    "        self.update.append(update)\n",
    "        self.entropy_loss.append(np.mean(self.entropy_loss_buffer))\n",
    "        self.critic_loss.append(np.mean(self.critic_loss_buffer))\n",
    "        self.actor_loss.append(np.mean(self.actor_loss_buffer))\n",
    "        self.ratio.append(np.mean(self.ratio_buffer))\n",
    "        self.entropy_loss_buffer = []\n",
    "        self.actor_loss_buffer = []\n",
    "        self.critic_loss_buffer = []\n",
    "        self.ratio_buffer = []\n",
    "\n",
    "\n",
    "    def plot_stats(self):\n",
    "        return self.rewards #TODO\n",
    "\n",
    "    def display_stats(self):\n",
    "        return print(self.rewards) #TODO\n",
    "\n",
    "    def update_mxboard(self):\n",
    "        self.sw.add_scalar(tag='Losses/Critic_Loss',                           value=self.critic_loss[-1],         global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Losses/Actor_Loss',                            value=self.actor_loss[-1],          global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Losses/Entropy_Loss',                          value=self.entropy_loss[-1],        global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Probabilities/Average_Action_Probability',     value=self.avg_action_prob[-1],     global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Probabilities/Min._Action_Probability',        value=self.min_action_prob[-1],     global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Probabilities/Max._Action_Probability',        value=self.max_action_prob[-1],     global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Probabilities/Std._Dev._Action_Probability',   value=self.std_action_prob[-1],     global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Values/Average_State_Value',                   value=self.avg_value[-1],           global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Values/Min._State_Value',                      value=self.min_value[-1],           global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Values/Max._State_Value',                      value=self.max_value[-1],           global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Values/Std._Dev._State_Value',                 value=self.std_value[-1],           global_step=self.update[-1])\n",
    "        self.sw.add_scalar(tag='Ratio/Maximum_Ratio',                          value=self.ratio[-1],               global_step=self.update[-1])\n",
    "        if self.mean100:\n",
    "            self.update_max_reward.append(self.max_reward[-1])\n",
    "            self.update_rewards.append(self.rewards[-1])\n",
    "            self.update_mean100.append(self.mean100[-1])\n",
    "            self.sw.add_scalar(tag='Rewards/Rewards',                              value=self.rewards[-1],             global_step=self.update[-1])\n",
    "            self.sw.add_scalar(tag='Rewards/Avg._Reward_Last_100',                 value=self.mean100[-1],             global_step=self.update[-1])\n",
    "            self.sw.add_scalar(tag='Rewards/Max._Reward',                          value=self.max_reward[-1],          global_step=self.update[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9mnjLr_kFo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "\n",
    "def preprocess(state):\n",
    "    state = cv2.resize(cv2.cvtColor(state, cv2.COLOR_RGB2GRAY), (h, w), interpolation=cv2.INTER_AREA)\n",
    "    state = state / 255\n",
    "    return state\n",
    "\n",
    "def summarize(net, context):\n",
    "    state = env.reset()\n",
    "    state = preprocess(state)\n",
    "    state = np.array([state,]*stacked_frames).reshape((1, 1,stacked_frames, h, w))\n",
    "    net.summary(nd.array(state, ctx=context))\n",
    "\n",
    "def reset_state(runner):\n",
    "    runner.child.send((\"reset\", None))\n",
    "    state = runner.child.recv()\n",
    "    state = preprocess(state)\n",
    "    state = np.array([state,]*stacked_frames)\n",
    "    return state\n",
    "\n",
    "def process_states(states_new, states, dones, infos, runners, game, lives):\n",
    "    global cur_eps\n",
    "    global total_episodes\n",
    "    global monitor\n",
    "    states_new = np.stack(states_new)\n",
    "    states_new = np.array([preprocess(state) for state in states_new])\n",
    "    states = np.append(states, states_new.reshape(num_workers,1,1,h,w), axis=2)\n",
    "    states = np.delete(states, 0, axis=2)\n",
    "    for idx, [cur_done, runner, cur_info] in enumerate(zip(dones, runners, infos)):\n",
    "        if cur_done or (cur_info['ale.lives'] < lives[idx]):\n",
    "            lives[idx] = cur_info['ale.lives']\n",
    "            if cur_done:\n",
    "                monitor.process_episode(cur_eps[idx], total_episodes)\n",
    "                total_episodes += 1\n",
    "                cur_eps[idx] = 0\n",
    "                lives[idx] = env.unwrapped.ale.lives()\n",
    "                states[idx] = reset_state(runner)\n",
    "            dones[idx] = True\n",
    "    return states, dones\n",
    "\n",
    "def standardize(adv):\n",
    "    return (adv - adv.mean()) / (nd.sqrt(nd.power(adv-adv.mean(),2).sum() / len(adv)) + 1e-10)\n",
    "\n",
    "def calculate_advantages(done, rewards, values, states, hidden):\n",
    "    advantages = np.zeros((values.shape[0], values.shape[1]), dtype=np.float32)\n",
    "    next_advantage = 0\n",
    "\n",
    "    # Get the value of the state that resulted from the last action\n",
    "    _, next_value, *_ = net_new(nd.array(states, ctx=context, dtype=np.float32).reshape(num_workers, 1, stacked_frames*c, h, w),hidden)\n",
    "    next_value = next_value.reshape(-1).asnumpy()\n",
    "\n",
    "    # Work backwards through values to calculate GAE\n",
    "    # Whenever a life was lost or an episode ended it will have been marked with done\n",
    "    # Using this as a mask allows us to restart advantage calculation from these points\n",
    "    for t in reversed(range(values.shape[1])):\n",
    "        mask = 1.0 - done[:, t]\n",
    "        delta = rewards[:, t] + gamma * next_value * mask - values[:, t]\n",
    "        advantages[:, t] = delta + gamma * lamda * next_advantage * mask\n",
    "\n",
    "        next_advantage = advantages[:, t]\n",
    "        next_value = values[:, t]\n",
    "\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-J3XrrBkFo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "def calc_actor_loss(probs, mini_batch, clip_range, advantages):\n",
    "\n",
    "    # Get log probabilities and PPO style clip the ratio between original model and current output\n",
    "    log_probs = nd.log(nd.pick(probs,mini_batch['actions'])+1e-10)\n",
    "    ratio = nd.exp(log_probs - mini_batch['log_probs'])\n",
    "    clipped_ratio = nd.clip(ratio,1.0 - clip_range,1.0 + clip_range)\n",
    "\n",
    "    # Take the minimum result of the base ratio and the clipped ratio\n",
    "    actor_loss = nd.concat((ratio * advantages.detach()).reshape(1,-1),(clipped_ratio * advantages.detach()).reshape(1,-1), dim = 0)\n",
    "    actor_loss = nd.min(actor_loss, axis=0)\n",
    "    actor_loss = -actor_loss.mean()\n",
    "    return actor_loss, ratio\n",
    "\n",
    "def calc_critic_loss(mini_batch, value, clip_range):\n",
    "    # Calculate the batch return and clip the current value to stabilize the critic  \n",
    "    batch_return = mini_batch['values'] + mini_batch['advantages']\n",
    "    clipped_value = mini_batch['values'].detach() + nd.clip(value.reshape(-1) - mini_batch['values'].detach(), -clip_range, clip_range)\n",
    "    critic_loss = nd.abs(clipped_value - batch_return.detach())\n",
    "    critic_loss = critic_loss.mean()\n",
    "    return critic_loss\n",
    "\n",
    "def calc_entropy_loss(probs):\n",
    "    probs = probs+1e-10\n",
    "    entropy_loss = -(probs * probs.log()).sum(axis=1)\n",
    "    entropy_loss = entropy_loss.mean()\n",
    "    return entropy_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtZOp8fEkFo4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Batch rollout\n",
    "\n",
    "def rollout():\n",
    "    global states\n",
    "    global cur_eps\n",
    "    global lives\n",
    "    global hidden\n",
    "\n",
    "    # Initialize batch\n",
    "    data = {'rewards':         np.zeros((num_workers, batch_steps)),\\\n",
    "            'values':          np.zeros((num_workers, batch_steps)),\\\n",
    "            'log_probs':       np.zeros((num_workers, batch_steps)),\\\n",
    "            'action_dists':    np.zeros((num_workers, batch_steps, env.action_space.n)),\\\n",
    "            'done':            np.zeros((num_workers, batch_steps)),\\\n",
    "            'states':          np.zeros((num_workers, batch_steps, stacked_frames * c, h, w)),\\\n",
    "            'actions':         np.zeros((num_workers, batch_steps), dtype=np.int32),\\\n",
    "            'rnn_inputs':      np.zeros((num_workers, batch_steps, hidden_size)),\\\n",
    "            'hiddens':         np.zeros((num_workers, batch_steps, 2, rnn_hidden_size))}\n",
    "\n",
    "    for step in range(batch_steps):\n",
    "\n",
    "        # Store hidden states\n",
    "        data['hiddens'][:, step, 0] = hidden[0].asnumpy()\n",
    "        data['hiddens'][:, step, 1] = hidden[1].asnumpy()\n",
    "\n",
    "        # Forward pass\n",
    "        probs, v, hidden, rnn_input, rnn_output, *_ = net(nd.array(states, ctx=context, dtype=np.float32),hidden)\n",
    "\n",
    "        # Sample action\n",
    "        act = mx.nd.sample_multinomial(probs)\n",
    "        act_probs = nd.pick(probs, act)\n",
    "        log_probs = nd.log(act_probs+1e-10)\n",
    "\n",
    "        # Store the new values in the batch data\n",
    "        data['rnn_inputs'][:, step] = rnn_input.asnumpy().reshape(num_workers, -1) # step-1 because every rnn_input will be the t+1 target for input prediction\n",
    "        data['states'][:, step]       = states.reshape(num_workers, stacked_frames * c, h, w)\n",
    "        data['action_dists'][:, step] = probs.asnumpy()\n",
    "        data['values'][:, step]       = v.reshape(-1).asnumpy()\n",
    "        data['actions'][:, step]      = act.reshape(-1).asnumpy()\n",
    "        data['log_probs'][:, step]    = log_probs.reshape(-1).asnumpy()\n",
    "\n",
    "        # Execute the chosen actions in the workers and retrieve + process the next states\n",
    "        for idx, runner in enumerate(runners):\n",
    "            runner.child.send((\"step\", data['actions'][idx,step]))\n",
    "        states_new, data['rewards'][:, step], data['done'][:, step], info = np.transpose(np.array([runner.child.recv() for runner in runners], dtype='object'))\n",
    "        cur_eps = cur_eps + data['rewards'][:, step] # Track cumulative reward of the running episodes\n",
    "        states, data['done'][:, step] = process_states(states_new, states, data['done'][:, step], info, runners, game, lives)\n",
    "\n",
    "    # Process rollout for monitoring and convert to mx ndarray for backwards pass\n",
    "    monitor.process_rollout(data)\n",
    "    for key, val in data.items():\n",
    "        data[key] = nd.array(val, ctx=context, dtype=np.float32)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SiuT3MHakFo5",
    "outputId": "4b5fd923-4d4c-4b93-fdbf-230263e1aa28",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model and prepare for main loop\n",
    "\n",
    "net = Model()\n",
    "net.initialize(ctx=context)\n",
    "optimizer = gluon.Trainer(net.collect_params(), 'Adam', {'learning_rate': opt_lr, 'epsilon' : opt_eps})\n",
    "net_new = Model()\n",
    "net_new.initialize(ctx=context)\n",
    "\n",
    "optimizer_new = gluon.Trainer(net_new.collect_params(), 'Adam', {'learning_rate': opt_lr, 'epsilon' : opt_eps})\n",
    "monitor = Monitoring(output_dir)\n",
    "hidden = mx.nd.random.uniform(shape=(num_workers, rnn_hidden_size), ctx=context, dtype=np.float32)\n",
    "hidden = [hidden, hidden] # Gluon LSTM expects a list of recurrent state tensors (h0, c0)\n",
    "\n",
    "net.summary(nd.array(states, ctx=context, dtype=np.float32), hidden)\n",
    "probs, value, _, rnn_input, rnn_output = net_new(nd.array(states, ctx=context, dtype=np.float32), hidden)\n",
    "\n",
    "# Copy parameters between nets\n",
    "params1 = net_new.collect_params()\n",
    "params2 = net.collect_params()\n",
    "for p1, p2 in zip(params1.values(), params2.values()):\n",
    "    p2.set_data(p1.data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lq8dOBLkFo6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize worker processes\n",
    "\n",
    "runners = [Runner(game) for i in range(num_workers)]\n",
    "for i, runner in enumerate(runners):\n",
    "    states[i] = reset_state(runner).reshape(1,1,stacked_frames * c,h,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b7a9f9b219ce4416b16e45a0a0ed83a7",
      "409dca85968243b8a5c56d64c1bd9b34",
      "4374004e363f47e5ba4fb8603f68a420",
      "3e52e63d730c446390191e0336a0c5ec",
      "c4781b64f61a414b99f3ced953cca720",
      "99d8c72a30924291852e7de146d02db0",
      "9417998d438d4f55919dad8217a7eecb",
      "72f1c3e5a5fc42ba942a669186e01f4b"
     ]
    },
    "id": "sVprN465kFo6",
    "outputId": "60775e85-1877-4fee-d36d-31b555f37fb0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Main loop\n",
    "\n",
    "with tqdm(range(schedule_steps+cooldown_period), desc='Training..') as updates:\n",
    "\n",
    "    for update in updates:\n",
    "\n",
    "        # Update learning rate\n",
    "        if update<schedule_steps:\n",
    "            progress = update/schedule_steps\n",
    "            opt_lr = 2.5e-4 * (1 - progress)\n",
    "\n",
    "        # Fetch new batch of data\n",
    "        batch = rollout()\n",
    "        batch['new_values'] = batch['values']\n",
    "        batch['advantages'] = nd.zeros((num_workers,batch_steps), ctx=context, dtype=np.float32)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "\n",
    "            for mbatch in range(0, n_mini_batch):\n",
    "                # Copy parameters between nets\n",
    "                params1 = net_new.collect_params()\n",
    "                params2 = net.collect_params()\n",
    "                for p1, p2 in zip(params1.values(), params2.values()):\n",
    "                    p2.set_data(p1.data())\n",
    "\n",
    "                    \n",
    "                first = mbatch * mini_work_size\n",
    "                final = first + mini_work_size\n",
    "\n",
    "                # Create and fill new mini_batch\n",
    "                mini_batch = {}\n",
    "                \n",
    "                # Reshape states to avoid mxnet slice dimension limit\n",
    "                for key, val in batch.items():\n",
    "                    mini_batch[key] = val[:,first:final]\n",
    "\n",
    "                # Retrieve relevant hidden states (only the first, to minimize usage of stale hidden states)\n",
    "                mb_initial_hidden = [mini_batch['hiddens'][:, 0, 0].detach(), mini_batch['hiddens'][:,0, 1].detach()]\n",
    "\n",
    "\n",
    "\n",
    "                with autograd.record():\n",
    "\n",
    "                    # Get updated outputs with latest model\n",
    "                    probs, value, _, rnn_input, rnn_output = net_new(mini_batch['states'], mb_initial_hidden)\n",
    "                    batch['new_values'][:,mbatch*mini_work_size:mbatch*mini_work_size+mini_work_size] = value.reshape(num_workers,-1).detach()\n",
    "\n",
    "                    batch['advantages'][:,first:] = nd.array(calculate_advantages(batch['done'][:,first:].asnumpy(), batch['rewards'][:,first:].asnumpy(), batch['new_values'][:,first:].asnumpy(), states, hidden), ctx=context)\n",
    "                    mini_batch['advantages'] = batch['advantages'][:,first:final]\n",
    "                    \n",
    "                    # Concatenate all worker data for further loss calculation\n",
    "                    for key, val in mini_batch.items():\n",
    "                        mini_batch[key] = val.reshape(val.shape[0] * val.shape[1], *val.shape[2:])\n",
    "\n",
    "                    standardized_adv = standardize(mini_batch['advantages'])\n",
    "                    mb_hidden = [mini_batch['hiddens'][:, 0].detach(), mini_batch['hiddens'][:, 1].detach()] # Gluon LSTM format\n",
    "\n",
    "                    # Calculate losses\n",
    "                    actor_loss, ratio  = calc_actor_loss(probs, mini_batch, clip_range, standardized_adv)\n",
    "                    critic_loss  = calc_critic_loss(mini_batch, value, clip_range)\n",
    "                    entropy_loss = calc_entropy_loss(probs)\n",
    "\n",
    "                    # Total loss\n",
    "                    loss = actor_coeff * actor_loss \\\n",
    "                         + critic_coeff * critic_loss  \\\n",
    "                         - entropy_coeff * entropy_loss\n",
    "\n",
    "                    optimizer_new.set_learning_rate(opt_lr)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                grads = [i.grad(context) for i in net_new.collect_params().values() if i._grad is not None]\n",
    "                gluon.utils.clip_global_norm(grads, opt_clip)\n",
    "                optimizer_new.step(1)\n",
    "\n",
    "                # Update Tensorboard and logging\n",
    "                monitor.process_minibatch_loss(entropy_coeff * entropy_loss.asnumpy()[0],\\\n",
    "                                               actor_coeff * actor_loss.asnumpy()[0],\\\n",
    "                                               critic_coeff * critic_loss.asnumpy()[0],\\\n",
    "                                               np.max(ratio.asnumpy()))\n",
    "\n",
    "        monitor.process_update(update)\n",
    "        monitor.update_mxboard()\n",
    "        if monitor.mean100: \n",
    "            updates.set_description('Training.. (Avg. last 100: %.2f)' % monitor.mean100[-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytKXwyZikFo6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_data = monitor.__dict__.copy()\n",
    "del save_data[\"sw\"]\n",
    "pickle.dump(save_data, open( output_dir + \"/monitor.pkl\", \"wb\" ) )\n",
    "net.save_parameters(output_dir + \"/net.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "speed_single_fixed?_nodecoder.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "pytorch-gpu.1-4.m58",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m58"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3e52e63d730c446390191e0336a0c5ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72f1c3e5a5fc42ba942a669186e01f4b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9417998d438d4f55919dad8217a7eecb",
      "value": " 0/17000 [00:00&lt;?, ?it/s]"
     }
    },
    "409dca85968243b8a5c56d64c1bd9b34": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4374004e363f47e5ba4fb8603f68a420": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Training..:   0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99d8c72a30924291852e7de146d02db0",
      "max": 17000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4781b64f61a414b99f3ced953cca720",
      "value": 0
     }
    },
    "72f1c3e5a5fc42ba942a669186e01f4b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9417998d438d4f55919dad8217a7eecb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99d8c72a30924291852e7de146d02db0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7a9f9b219ce4416b16e45a0a0ed83a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4374004e363f47e5ba4fb8603f68a420",
       "IPY_MODEL_3e52e63d730c446390191e0336a0c5ec"
      ],
      "layout": "IPY_MODEL_409dca85968243b8a5c56d64c1bd9b34"
     }
    },
    "c4781b64f61a414b99f3ced953cca720": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}